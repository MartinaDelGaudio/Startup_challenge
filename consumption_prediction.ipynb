{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data['hour'] = data['time'].dt.hour\n",
    "data['day_of_week'] = data['time'].dt.dayofweek\n",
    "data['month'] = data['time'].dt.month\n",
    "In [5]:\n",
    "# Remove rows with double decimal points\n",
    "data = data[data['energy(kWh/hh)'].apply(lambda x: len(x.split('.')) == 2 and '.' not in x.split('.')[1])]\n",
    "In [6]:\n",
    "# Columns for prediction\n",
    "features = ['hour', 'day_of_week', 'month', 'temperature', 'humidity']\n",
    "target = 'energy(kWh/hh)'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "In [7]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "In [9]:\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "\n",
    "print(\"Neural Network RMSE:\", rmse_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/100\n",
    "/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
    "  super().__init__(name, **kwargs)\n",
    "10197/10197 [==============================] - 7s 693us/step - loss: 0.1608 - val_loss: 0.1636\n",
    "Epoch 2/100\n",
    "10197/10197 [==============================] - 7s 694us/step - loss: 0.1591 - val_loss: 0.1625\n",
    "Epoch 3/100\n",
    "10197/10197 [==============================] - 9s 879us/step - loss: 0.1581 - val_loss: 0.1621\n",
    "Epoch 4/100\n",
    "10197/10197 [==============================] - 7s 696us/step - loss: 0.1577 - val_loss: 0.1613\n",
    "Epoch 5/100\n",
    "10197/10197 [==============================] - 7s 664us/step - loss: 0.1575 - val_loss: 0.1617\n",
    "Epoch 6/100\n",
    "10197/10197 [==============================] - 7s 713us/step - loss: 0.1575 - val_loss: 0.1612\n",
    "Epoch 7/100\n",
    "10197/10197 [==============================] - 7s 682us/step - loss: 0.1574 - val_loss: 0.1616\n",
    "Epoch 8/100\n",
    "10197/10197 [==============================] - 7s 656us/step - loss: 0.1574 - val_loss: 0.1615\n",
    "Epoch 9/100\n",
    "10197/10197 [==============================] - 7s 732us/step - loss: 0.1573 - val_loss: 0.1619\n",
    "Epoch 10/100\n",
    "10197/10197 [==============================] - 7s 688us/step - loss: 0.1573 - val_loss: 0.1613\n",
    "Epoch 11/100\n",
    "10197/10197 [==============================] - 7s 717us/step - loss: 0.1572 - val_loss: 0.1612\n",
    "Epoch 12/100\n",
    "10197/10197 [==============================] - 7s 677us/step - loss: 0.1572 - val_loss: 0.1613\n",
    "Epoch 13/100\n",
    "10197/10197 [==============================] - 8s 831us/step - loss: 0.1572 - val_loss: 0.1612\n",
    "Epoch 14/100\n",
    "10197/10197 [==============================] - 7s 656us/step - loss: 0.1572 - val_loss: 0.1613\n",
    "Epoch 15/100\n",
    "10197/10197 [==============================] - 7s 667us/step - loss: 0.1572 - val_loss: 0.1618\n",
    "Epoch 16/100\n",
    "10197/10197 [==============================] - 7s 667us/step - loss: 0.1572 - val_loss: 0.1611\n",
    "Epoch 17/100\n",
    "10197/10197 [==============================] - 8s 743us/step - loss: 0.1571 - val_loss: 0.1612\n",
    "Epoch 18/100\n",
    "10197/10197 [==============================] - 8s 742us/step - loss: 0.1571 - val_loss: 0.1611\n",
    "Epoch 19/100\n",
    "10197/10197 [==============================] - 7s 665us/step - loss: 0.1571 - val_loss: 0.1619\n",
    "Epoch 20/100\n",
    "10197/10197 [==============================] - 7s 699us/step - loss: 0.1571 - val_loss: 0.1613\n",
    "Epoch 21/100\n",
    "10197/10197 [==============================] - 7s 662us/step - loss: 0.1571 - val_loss: 0.1612\n",
    "Epoch 22/100\n",
    "10197/10197 [==============================] - 7s 664us/step - loss: 0.1571 - val_loss: 0.1613\n",
    "Epoch 23/100\n",
    "10197/10197 [==============================] - 8s 775us/step - loss: 0.1571 - val_loss: 0.1611\n",
    "Epoch 24/100\n",
    "10197/10197 [==============================] - 7s 656us/step - loss: 0.1571 - val_loss: 0.1616\n",
    "Epoch 25/100\n",
    "10197/10197 [==============================] - 7s 648us/step - loss: 0.1571 - val_loss: 0.1613\n",
    "Epoch 26/100\n",
    "10197/10197 [==============================] - 7s 647us/step - loss: 0.1571 - val_loss: 0.1614\n",
    "Epoch 27/100\n",
    "10197/10197 [==============================] - 7s 668us/step - loss: 0.1571 - val_loss: 0.1613\n",
    "Epoch 28/100\n",
    "10197/10197 [==============================] - 7s 655us/step - loss: 0.1571 - val_loss: 0.1615\n",
    "Epoch 29/100\n",
    "10197/10197 [==============================] - 7s 660us/step - loss: 0.1570 - val_loss: 0.1611\n",
    "Epoch 30/100\n",
    "10197/10197 [==============================] - 7s 646us/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 31/100\n",
    "10197/10197 [==============================] - 7s 705us/step - loss: 0.1570 - val_loss: 0.1614\n",
    "Epoch 32/100\n",
    "10197/10197 [==============================] - 7s 703us/step - loss: 0.1570 - val_loss: 0.1613\n",
    "Epoch 33/100\n",
    "10197/10197 [==============================] - 7s 683us/step - loss: 0.1570 - val_loss: 0.1613\n",
    "Epoch 34/100\n",
    "10197/10197 [==============================] - 8s 755us/step - loss: 0.1570 - val_loss: 0.1611\n",
    "Epoch 35/100\n",
    "10197/10197 [==============================] - 29s 3ms/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 36/100\n",
    "10197/10197 [==============================] - 7s 717us/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 37/100\n",
    "10197/10197 [==============================] - 7s 695us/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 38/100\n",
    "10197/10197 [==============================] - 8s 743us/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 39/100\n",
    "10197/10197 [==============================] - 7s 689us/step - loss: 0.1570 - val_loss: 0.1611\n",
    "Epoch 40/100\n",
    "10197/10197 [==============================] - 7s 673us/step - loss: 0.1570 - val_loss: 0.1622\n",
    "Epoch 41/100\n",
    "10197/10197 [==============================] - 7s 721us/step - loss: 0.1569 - val_loss: 0.1615\n",
    "Epoch 42/100\n",
    "10197/10197 [==============================] - 8s 739us/step - loss: 0.1569 - val_loss: 0.1620\n",
    "Epoch 43/100\n",
    "10197/10197 [==============================] - 7s 664us/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 44/100\n",
    "10197/10197 [==============================] - 7s 695us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 45/100\n",
    "10197/10197 [==============================] - 7s 694us/step - loss: 0.1570 - val_loss: 0.1612\n",
    "Epoch 46/100\n",
    "10197/10197 [==============================] - 9s 866us/step - loss: 0.1570 - val_loss: 0.1613\n",
    "Epoch 47/100\n",
    "10197/10197 [==============================] - 7s 710us/step - loss: 0.1570 - val_loss: 0.1613\n",
    "Epoch 48/100\n",
    "10197/10197 [==============================] - 7s 713us/step - loss: 0.1570 - val_loss: 0.1611\n",
    "Epoch 49/100\n",
    "10197/10197 [==============================] - 7s 661us/step - loss: 0.1569 - val_loss: 0.1610\n",
    "Epoch 50/100\n",
    "10197/10197 [==============================] - 7s 693us/step - loss: 0.1570 - val_loss: 0.1611\n",
    "Epoch 51/100\n",
    "10197/10197 [==============================] - 7s 692us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 52/100\n",
    "10197/10197 [==============================] - 7s 674us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 53/100\n",
    "10197/10197 [==============================] - 7s 687us/step - loss: 0.1569 - val_loss: 0.1615\n",
    "Epoch 54/100\n",
    "10197/10197 [==============================] - 7s 683us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 55/100\n",
    "10197/10197 [==============================] - 7s 687us/step - loss: 0.1569 - val_loss: 0.1618\n",
    "Epoch 56/100\n",
    "10197/10197 [==============================] - 7s 666us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 57/100\n",
    "10197/10197 [==============================] - 8s 807us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 58/100\n",
    "10197/10197 [==============================] - 7s 732us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 59/100\n",
    "10197/10197 [==============================] - 7s 726us/step - loss: 0.1569 - val_loss: 0.1613\n",
    "Epoch 60/100\n",
    "10197/10197 [==============================] - 7s 676us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 61/100\n",
    "10197/10197 [==============================] - 8s 780us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 62/100\n",
    "10197/10197 [==============================] - 8s 746us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 63/100\n",
    "10197/10197 [==============================] - 7s 680us/step - loss: 0.1569 - val_loss: 0.1614\n",
    "Epoch 64/100\n",
    "10197/10197 [==============================] - 8s 743us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 65/100\n",
    "10197/10197 [==============================] - 7s 708us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 66/100\n",
    "10197/10197 [==============================] - 8s 745us/step - loss: 0.1569 - val_loss: 0.1613\n",
    "Epoch 67/100\n",
    "10197/10197 [==============================] - 7s 709us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 68/100\n",
    "10197/10197 [==============================] - 7s 706us/step - loss: 0.1569 - val_loss: 0.1615\n",
    "Epoch 69/100\n",
    "10197/10197 [==============================] - 7s 666us/step - loss: 0.1569 - val_loss: 0.1615\n",
    "Epoch 70/100\n",
    "10197/10197 [==============================] - 7s 696us/step - loss: 0.1569 - val_loss: 0.1615\n",
    "Epoch 71/100\n",
    "10197/10197 [==============================] - 7s 676us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 72/100\n",
    "10197/10197 [==============================] - 7s 676us/step - loss: 0.1569 - val_loss: 0.1613\n",
    "Epoch 73/100\n",
    "10197/10197 [==============================] - 7s 704us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 74/100\n",
    "10197/10197 [==============================] - 7s 663us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 75/100\n",
    "10197/10197 [==============================] - 7s 699us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 76/100\n",
    "10197/10197 [==============================] - 7s 662us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 77/100\n",
    "10197/10197 [==============================] - 7s 690us/step - loss: 0.1569 - val_loss: 0.1616\n",
    "Epoch 78/100\n",
    "10197/10197 [==============================] - 7s 718us/step - loss: 0.1569 - val_loss: 0.1614\n",
    "Epoch 79/100\n",
    "10197/10197 [==============================] - 7s 735us/step - loss: 0.1569 - val_loss: 0.1617\n",
    "Epoch 80/100\n",
    "10197/10197 [==============================] - 7s 680us/step - loss: 0.1569 - val_loss: 0.1610\n",
    "Epoch 81/100\n",
    "10197/10197 [==============================] - 7s 672us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 82/100\n",
    "10197/10197 [==============================] - 7s 690us/step - loss: 0.1569 - val_loss: 0.1613\n",
    "Epoch 83/100\n",
    "10197/10197 [==============================] - 7s 665us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 84/100\n",
    "10197/10197 [==============================] - 7s 723us/step - loss: 0.1569 - val_loss: 0.1616\n",
    "Epoch 85/100\n",
    "10197/10197 [==============================] - 7s 671us/step - loss: 0.1569 - val_loss: 0.1615\n",
    "Epoch 86/100\n",
    "10197/10197 [==============================] - 8s 762us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 87/100\n",
    "10197/10197 [==============================] - 7s 707us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 88/100\n",
    "10197/10197 [==============================] - 7s 725us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 89/100\n",
    "10197/10197 [==============================] - 8s 760us/step - loss: 0.1569 - val_loss: 0.1616\n",
    "Epoch 90/100\n",
    "10197/10197 [==============================] - 7s 668us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 91/100\n",
    "10197/10197 [==============================] - 7s 689us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 92/100\n",
    "10197/10197 [==============================] - 7s 660us/step - loss: 0.1569 - val_loss: 0.1613\n",
    "Epoch 93/100\n",
    "10197/10197 [==============================] - 7s 704us/step - loss: 0.1568 - val_loss: 0.1611\n",
    "Epoch 94/100\n",
    "10197/10197 [==============================] - 7s 679us/step - loss: 0.1568 - val_loss: 0.1611\n",
    "Epoch 95/100\n",
    "10197/10197 [==============================] - 7s 696us/step - loss: 0.1569 - val_loss: 0.1612\n",
    "Epoch 96/100\n",
    "10197/10197 [==============================] - 7s 660us/step - loss: 0.1569 - val_loss: 0.1613\n",
    "Epoch 97/100\n",
    "10197/10197 [==============================] - 7s 680us/step - loss: 0.1569 - val_loss: 0.1616\n",
    "Epoch 98/100\n",
    "10197/10197 [==============================] - 7s 665us/step - loss: 0.1569 - val_loss: 0.1611\n",
    "Epoch 99/100\n",
    "10197/10197 [==============================] - 7s 680us/step - loss: 0.1569 - val_loss: 0.1618\n",
    "Epoch 100/100\n",
    "10197/10197 [==============================] - 7s 701us/step - loss: 0.1568 - val_loss: 0.1613\n",
    "3187/3187 [==============================] - 1s 398us/step\n",
    "Neural Network RMSE: 0.40195644541639003"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
